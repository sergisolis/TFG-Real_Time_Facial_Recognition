{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as preprocess_inception_resnet_v2\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as preprocess_inception_v3\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobilenet_v2\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg16\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as preprocess_vgg19\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input as preprocess_mobilenet_v3\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficientnet\n",
    "\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocess_function(base_model_type):\n",
    "    if base_model_type == 'ResNet50':\n",
    "        return preprocess_resnet50\n",
    "    elif base_model_type == 'InceptionResNetV2':\n",
    "        return preprocess_inception_resnet_v2\n",
    "    elif base_model_type == 'InceptionV3':\n",
    "        return preprocess_inception_v3\n",
    "    elif base_model_type == 'MobileNetV2':\n",
    "        return preprocess_mobilenet_v2\n",
    "    elif base_model_type == 'MobileNetV3':\n",
    "        return preprocess_mobilenet_v3\n",
    "    elif base_model_type == 'VGG16':\n",
    "        return preprocess_vgg16\n",
    "    elif base_model_type == 'VGG19':\n",
    "        return preprocess_vgg19\n",
    "    elif base_model_type == 'ResNet50V2':\n",
    "        return preprocess_inception_resnet_v2\n",
    "    elif base_model_type == 'EfficientNet':\n",
    "        return preprocess_efficientnet\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Invalid base_model_type.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file, input_shape):\n",
    "    face = cv2.imread(file)\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "    face = cv2.resize(face, input_shape)\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_ref_embedding(model, input_shape, preprocess_input):\n",
    "    # Load embeddings set\n",
    "    test_dir = './ref_data/min_data'\n",
    "    test_embeddings = []\n",
    "    test_labels = []\n",
    "    for subdir in os.listdir(test_dir):\n",
    "        for file in glob.glob(os.path.join(test_dir, subdir, '*.jpg')):\n",
    "            face = preprocess(file, input_shape)\n",
    "            # Preprocess image same as training data\n",
    "            face = preprocess_input(face)\n",
    "\n",
    "            embedding = model.predict(face, verbose=0)\n",
    "            test_embeddings.append(embedding.flatten())\n",
    "            test_labels.append(subdir)\n",
    "\n",
    "    return test_embeddings, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(x):\n",
    "    return x / np.sqrt(np.sum(np.multiply(x, x)))\n",
    "\n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance\n",
    "\n",
    "def predict(feature, all_features, labels):\n",
    "    distances = np.sqrt(np.sum(np.square(feature - all_features), axis = 1))\n",
    "    index = np.argmin(distances)\n",
    "\n",
    "    name = labels[index]\n",
    "    distance = distances[index]\n",
    "    return name, distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL TIME TRIPLET LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recognize faces\n",
    "def recognize_faces(image, face_boxes, model, ref_features, ref_labels, preprocess_input, threshold=0.45):\n",
    "     recognized_faces = []\n",
    "     distances = []\n",
    "     for box in face_boxes:\n",
    "         x, y, width, height = box\n",
    "         face = image[y:y+height, x:x+width]\n",
    "\n",
    "         if face.size == 0: # If the face image is empty, skip this face\n",
    "             continue\n",
    "\n",
    "         face = cv2.resize(face, (224,224)) \n",
    "         face = preprocess_input(face)\n",
    "         face = np.expand_dims(face, axis=0)\n",
    "       \n",
    "         embedding = model.predict(face, verbose=0)\n",
    "         \n",
    "         predicted_label, distance = predict(embedding[0], ref_features, np.array(ref_labels))\n",
    "         print(predicted_label, distance)\n",
    "         if distance <= threshold:\n",
    "            recognized_faces.append(predicted_label)\n",
    "            distances.append(distance)\n",
    "            \n",
    "         else:\n",
    "            recognized_faces.append(\"3\")\n",
    "            distances.append(distance)\n",
    "         \n",
    "\n",
    "     return [recognized_faces, distances]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_TYPE = 'EfficientNet'\n",
    "\n",
    "model = load_model(\"triplet_loss_trained_models/\" + \"EfficientNet\" )\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-01)\n",
    "model.compile(optimizer=optimizer)\n",
    "preprocess_input = get_preprocess_function(BASE_MODEL_TYPE)\n",
    "ref_embeddings, ref_labels = import_ref_embedding(model, (224,224), preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6762822\n",
      "1 0.6146554\n",
      "1 0.8045504\n",
      "0 0.41431624\n",
      "0 0.32232332\n",
      "0 0.45401248\n",
      "1 0.7953351\n",
      "2 0.6980446\n",
      "2 0.5940066\n",
      "0 0.6692596\n",
      "0 0.3220486\n",
      "0 0.43268064\n",
      "0 0.3841468\n",
      "2 1.0813482\n",
      "2 0.6971509\n",
      "2 1.0820616\n",
      "2 0.91803443\n",
      "1 1.0088573\n",
      "2 0.8168856\n",
      "1 1.0025078\n",
      "2 0.91954404\n",
      "1 0.79533505\n",
      "2 0.92445254\n",
      "1 0.8231786\n",
      "1 0.8026843\n",
      "2 0.9379284\n",
      "1 0.8504975\n",
      "2 0.92943674\n",
      "1 0.9208703\n",
      "2 0.80041546\n",
      "1 0.74787515\n",
      "1 0.5494098\n",
      "0 0.4932686\n",
      "0 0.37488303\n",
      "1 0.92369896\n",
      "1 0.85063636\n",
      "2 0.9383852\n",
      "1 0.718068\n",
      "1 0.8446412\n",
      "1 0.8803817\n",
      "1 0.8614351\n",
      "1 0.68205714\n",
      "1 1.0134676\n",
      "1 0.58958405\n",
      "0 0.34409785\n",
      "1 1.019573\n",
      "1 0.8219104\n"
     ]
    }
   ],
   "source": [
    "# MEDIAPIPE Initialization\n",
    "import mediapipe as mp\n",
    "\n",
    "# Read the names from the text file into a list\n",
    "with open(\"names.txt\", \"r\") as file:\n",
    "    names = file.read().splitlines()\n",
    "\n",
    "preprocess_input = get_preprocess_function(BASE_MODEL_TYPE)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.2)\n",
    "\n",
    "# Webcam capture\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Counter for frames\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "          break\n",
    "\n",
    "      frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      result = face_detection.process(frame_rgb)\n",
    "\n",
    "      face_boxes = []\n",
    "      if result.detections:\n",
    "          for detection in result.detections:\n",
    "              bboxC = detection.location_data.relative_bounding_box\n",
    "              ih, iw, _ = frame.shape\n",
    "              x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "              face_boxes.append((x, y, w, h))\n",
    "              cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "      if frame_count % 50 == 0:\n",
    "        labels, distances = recognize_faces(frame, face_boxes, model, ref_embeddings, ref_labels, preprocess_input)\n",
    "\n",
    "      for label, distance, box in zip(labels, distances, face_boxes):\n",
    "            x, y, w, h = box\n",
    "            label_index = int(label)\n",
    "            distance = round(distance, 3)\n",
    "            if label_index <= 2:\n",
    "                name = names[label_index]\n",
    "                cv2.putText(frame, name + \" \" + str(distance), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            else:\n",
    "                name = \"unknown\"\n",
    "                cv2.putText(frame, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)  \n",
    "\n",
    "      cv2.imshow('Real-time Face Recognition', frame)\n",
    "\n",
    "      if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "          break\n",
    "      \n",
    "      frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL TIME CROSS ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recognize faces\n",
    "def recognize_faces(image, face_boxes, model, preprocess_input , threshold=0.90):\n",
    "     recognized_faces = []\n",
    "     probabilities = []\n",
    "     for box in face_boxes:\n",
    "         x, y, width, height = box\n",
    "         face = image[y:y+height, x:x+width]\n",
    "\n",
    "         if face.size == 0: # If the face image is empty, skip this face\n",
    "             continue\n",
    "\n",
    "         face = cv2.resize(face, (224,224)) \n",
    "         face = preprocess_input(face)\n",
    "         face = np.expand_dims(face, axis=0)\n",
    "       \n",
    "         predicted_probabilities = model.predict(face)\n",
    "         max_prob_index = np.argmax(predicted_probabilities, axis=1)\n",
    "         max_prob = np.max(predicted_probabilities, axis=1)\n",
    "         if max_prob >= threshold:\n",
    "             predicted_label = max_prob_index[0]\n",
    "         else:\n",
    "             predicted_label = \"3\"\n",
    "             max_prob = 0\n",
    "         recognized_faces.append(predicted_label)\n",
    "         probabilities.append(max_prob)\n",
    "         \n",
    "\n",
    "     return recognized_faces, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"cross_entropy_trained_models/\" + \"VGG16.h5\" )\n",
    "preprocess_input = get_preprocess_function(BASE_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "# MEDIAPIPE Initialization\n",
    "import mediapipe as mp\n",
    "\n",
    "# Read the names from the text file into a list\n",
    "with open(\"names.txt\", \"r\") as file:\n",
    "    names = file.read().splitlines()\n",
    "\n",
    "preprocess_input = get_preprocess_function(BASE_MODEL_TYPE)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.2)\n",
    "\n",
    "# Webcam capture\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Counter for frames\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "          break\n",
    "\n",
    "      frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      result = face_detection.process(frame_rgb)\n",
    "\n",
    "      face_boxes = []\n",
    "      if result.detections:\n",
    "          for detection in result.detections:\n",
    "              bboxC = detection.location_data.relative_bounding_box\n",
    "              ih, iw, _ = frame.shape\n",
    "              x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "              face_boxes.append((x, y, w, h))\n",
    "              cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "      if frame_count % 50 == 0:\n",
    "        labels, probabilities = recognize_faces(frame, face_boxes, model, preprocess_input)\n",
    "\n",
    "      for label, probability, box in zip(labels, probabilities, face_boxes):\n",
    "            x, y, w, h = box\n",
    "            label_index = int(label)\n",
    "            if label_index <= 2:\n",
    "                name = names[label_index]\n",
    "                cv2.putText(frame, name + \" \" + str(probability), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            else:\n",
    "                name = \"unknown\"\n",
    "                cv2.putText(frame, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)  \n",
    "\n",
    "      cv2.imshow('Real-time Face Recognition', frame)\n",
    "\n",
    "      if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "          break\n",
    "      \n",
    "      frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
